---
description: "Dependency-ordered task list for IPFS Upload and Batch Reveal Mechanism"
---

# Tasks: IPFS Upload and Batch Reveal Mechanism

**Input**: Design documents from `/specs/003-003d-ipfs-reveal/`
**Prerequisites**: plan.md, spec.md, research.md, data-model.md, contracts/

**Feature Branch**: `003-003d-ipfs-reveal`
**Dependencies**: 003a (backend foundation), 003b (event detection), 003c (image generation)

## Format: `[ID] [P?] [Story] Description`
- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (US1, US2, US3)
- Include exact file paths in descriptions

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and dependency setup

- [ ] T001 Add `requests` library to project dependencies via `uv add requests` in `backend/` directory
- [ ] T002 Create error hierarchy module `backend/src/glisk/services/exceptions.py` with base classes: `ServiceError`, `TransientError`, `PermanentError`, and service-specific exceptions for IPFS and blockchain
- [ ] T003 Create IPFS service directory structure: `backend/src/glisk/services/ipfs/` with `__init__.py`
- [ ] T004 Update configuration in `backend/src/glisk/core/config.py` to add Pinata settings (PINATA_JWT, PINATA_GATEWAY), keeper settings (KEEPER_PRIVATE_KEY, REVEAL_GAS_BUFFER, TRANSACTION_TIMEOUT_SECONDS), and worker settings (BATCH_REVEAL_WAIT_SECONDS, BATCH_REVEAL_MAX_TOKENS) with validation

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

**‚ö†Ô∏è ALEMBIC WORKFLOW**: Following constitution v1.1.0 - Models FIRST, then autogenerate migrations

- [ ] T005 Update SQLModel definitions in `backend/src/glisk/db/models.py` to add three new fields to `Token` model: `image_cid: Optional[str]`, `metadata_cid: Optional[str]`, `reveal_tx_hash: Optional[str]`
- [ ] T006 [P] Create `IPFSUploadRecord` model in `backend/src/glisk/db/models.py` with all fields matching schema: `upload_id`, `token_id`, `upload_type`, `ipfs_cid`, `status`, `attempt_number`, `error_message`, `created_at`
- [ ] T007 [P] Create `RevealTransaction` model in `backend/src/glisk/db/models.py` with all fields matching schema including PostgreSQL ARRAY columns: `reveal_id`, `tx_hash`, `token_ids`, `metadata_uris`, `status`, `gas_price`, `gas_limit`, `gas_used`, `block_number`, `error_message`, `submitted_at`, `confirmed_at`
- [ ] T008 [P] Autogenerate database migration using `uv run alembic revision --autogenerate -m "add_ipfs_reveal_fields"` to add three columns to `tokens_s0` table from T005 model changes
- [ ] T009 [P] Autogenerate database migration using `uv run alembic revision --autogenerate -m "create_ipfs_upload_records"` to create audit table from T006 model definition, manually add indexes on `token_id` and `(token_id, upload_type)` in generated migration
- [ ] T010 [P] Autogenerate database migration using `uv run alembic revision --autogenerate -m "create_reveal_transactions"` to create audit table from T007 model definition, manually add indexes on `tx_hash`, `status`, `submitted_at` in generated migration
- [ ] T011 Review all three autogenerated migrations for correctness: verify upgrade()/downgrade() logic, check ARRAY column handling, confirm UTC timestamps use `server_default=CURRENT_TIMESTAMP`, add missing indexes if autogenerate missed them
- [ ] T012 Apply all three migrations in order using `uv run alembic upgrade head` and verify schema changes using `psql` inspection commands
- [ ] T013 [P] Create repository `backend/src/glisk/repositories/ipfs_upload_record.py` with `IPFSUploadRecordRepository` class implementing `create()` method for audit record creation
- [ ] T014 [P] Create repository `backend/src/glisk/repositories/reveal_transaction.py` with `RevealTransactionRepository` class implementing `create()`, `mark_confirmed()`, and `mark_failed()` methods
- [ ] T015 Extend `TokenRepository` in `backend/src/glisk/repositories/token.py` with four new methods: `get_ready_for_upload()` (FOR UPDATE SKIP LOCKED), `update_ipfs_cids()`, `get_ready_for_reveal()` (FOR UPDATE SKIP LOCKED), `mark_revealed()`

**Checkpoint**: Foundation ready - models updated (source of truth), migrations autogenerated and applied, repositories ready, user story implementation can now begin

---

## Phase 3: User Story 1 - Automatic IPFS Upload (Priority: P1) üéØ MVP

**Goal**: Automatically upload generated images and metadata to IPFS so content is permanently stored and accessible via decentralized storage

**Independent Test**: Generate an image for a token, trigger upload process, verify both image and metadata CIDs are stored in token record and accessible via IPFS gateway URLs

### Implementation for User Story 1

- [ ] T016 [US1] Implement `PinataClient` class in `backend/src/glisk/services/ipfs/pinata_client.py` with constructor accepting `jwt_token` and `gateway_domain` parameters
- [ ] T017 [US1] Implement `upload_image()` method in `PinataClient` to download image from URL, POST to Pinata `/pinning/pinFileToIPFS` endpoint, parse CID from response, classify errors as TransientError (429, 500, 503, timeout) or PermanentError (400, 401, 403)
- [ ] T018 [US1] Implement `upload_metadata()` method in `PinataClient` to POST JSON to Pinata `/pinning/pinJSONToIPFS` endpoint with `pinataContent`, `pinataMetadata`, and `pinataOptions` (cidVersion: 1), parse CID from response, use same error classification
- [ ] T019 [US1] Implement `get_gateway_url()` helper method in `PinataClient` to convert CID to full gateway URL format `https://{gateway_domain}/ipfs/{cid}`
- [ ] T020 [US1] Create inline `build_metadata()` function in `backend/src/glisk/workers/ipfs_upload_worker.py` (not separate service) that accepts `token`, `image_cid`, `author` and returns ERC721 metadata dict with keys: `name` (f"Token #{token_id}"), `description` (includes author prompt), `image` (ipfs://{image_cid}), `attributes` (empty list for MVP)
- [ ] T021 [US1] Create IPFS upload worker module `backend/src/glisk/workers/ipfs_upload_worker.py` with main entry point `run_ipfs_upload_worker()` function implementing infinite polling loop with error handling and graceful shutdown on asyncio.CancelledError
- [ ] T022 [US1] Implement token processing function `process_upload_token()` in IPFS upload worker that polls `get_ready_for_upload()` (status='uploading', attempts < 3), processes each token in session-per-token pattern: upload image ‚Üí build metadata ‚Üí upload metadata ‚Üí update token with CIDs and status='ready' ‚Üí create audit records
- [ ] T023 [US1] Implement retry logic in IPFS upload worker: catch TransientError ‚Üí increment `generation_attempts` ‚Üí apply exponential backoff ‚Üí continue loop; catch PermanentError ‚Üí update `generation_error` ‚Üí set status='failed' ‚Üí create audit record with error
- [ ] T024 [US1] Add structured logging to IPFS upload worker for events: `worker.started`, `ipfs.image_uploaded`, `ipfs.metadata_uploaded`, `ipfs.transient_error`, `ipfs.permanent_error`, `worker.stopped`, include fields: token_id, cid, attempt_number, duration_seconds
- [ ] T025 [US1] Register IPFS upload worker in `backend/src/glisk/main.py` lifespan context to start background task on app startup and cancel on shutdown
- [ ] T026 [US1] Export `PinataClient`, `run_ipfs_upload_worker` in `backend/src/glisk/workers/__init__.py` for easy import

**Checkpoint**: At this point, User Story 1 should be fully functional - tokens with images are automatically uploaded to IPFS and transitioned to 'ready' status

---

## Phase 4: User Story 2 - Automatic Batch Reveal (Priority: P1) üéØ MVP

**Goal**: Automatically batch tokens with IPFS metadata and reveal them on-chain so users can see their NFTs with proper metadata

**Independent Test**: Create multiple tokens with status='ready' and valid metadata CIDs, trigger reveal worker, verify single batch transaction reveals all tokens on-chain with reveal transaction hash recorded

### Implementation for User Story 2

- [ ] T027 [US2] Implement `KeeperService` class in `backend/src/glisk/services/blockchain/keeper.py` with constructor accepting `w3` (Web3 instance), `contract_address`, `keeper_private_key`, `gas_buffer_percentage` (default 0.20), `transaction_timeout` (default 180)
- [ ] T028 [US2] Implement `estimate_gas()` method in `KeeperService` to call `w3.eth.estimate_gas()` for revealBatch transaction, fetch EIP-1559 parameters (`w3.eth.max_priority_fee`, `w3.eth.get_block('latest')['baseFeePerGas']`), apply gas buffer multiplier, return tuple (gas_limit, max_fee_per_gas, max_priority_fee_per_gas)
- [ ] T029 [US2] Implement `reveal_batch()` method in `KeeperService` to build transaction dict with token_ids and metadata_uris parameters, estimate gas internally, get nonce via `w3.eth.get_transaction_count(keeper_address, 'pending')`, sign transaction, send raw transaction, wait for receipt with timeout using `w3.eth.wait_for_transaction_receipt()`, check receipt.status (0=reverted, 1=success), return tuple (tx_hash, block_number, gas_used) or raise TransientError/PermanentError
- [ ] T030 [US2] Implement `get_keeper_address()` helper method in `KeeperService` to derive and return checksummed address from keeper private key
- [ ] T031 [US2] Add transaction revert detection in `KeeperService` to check `receipt.status == 0` and raise `PermanentError` with revert reason extracted from receipt
- [ ] T032 [US2] Create reveal worker module `backend/src/glisk/workers/reveal_worker.py` with main entry point `run_reveal_worker()` function implementing infinite polling loop with error handling and graceful shutdown
- [ ] T033 [US2] Implement batch accumulation function `get_batch()` in reveal worker using two-query pattern: initial lock up to `batch_max_size` tokens via `get_ready_for_reveal()`, if 0 < count < max, wait `batch_wait_time` seconds, poll again for remaining slots, return combined list
- [ ] T034 [US2] Implement batch processing function `process_reveal_batch()` in reveal worker that calls `get_batch()`, builds token_ids and metadata_uris arrays (format: `ipfs://{metadata_cid}`), calls `keeper.reveal_batch()`, creates reveal_transactions audit record with status='pending', waits for confirmation, marks record confirmed, updates all tokens in batch to status='revealed' with reveal_tx_hash
- [ ] T035 [US2] Implement error handling in reveal worker: catch TransientError (gas estimation, submission, timeout) ‚Üí log warning ‚Üí tokens remain 'ready' ‚Üí retry next poll; catch PermanentError (revert) ‚Üí log error ‚Üí create failed audit record ‚Üí tokens remain 'ready' ‚Üí manual investigation required
- [ ] T036 [US2] Add structured logging to reveal worker for events: `worker.started`, `reveal.gas_estimated`, `reveal.batch_submitted`, `reveal.batch_confirmed`, `reveal.transient_error`, `reveal.permanent_error`, `worker.stopped`, include fields: token_count, tx_hash, block_number, gas_used, gas_limit, batch_size, duration_seconds
- [ ] T037 [US2] Register reveal worker in `backend/src/glisk/main.py` lifespan context to start background task on app startup and cancel on shutdown
- [ ] T038 [US2] Export `KeeperService`, `run_reveal_worker` in `backend/src/glisk/workers/__init__.py` for easy import

**Checkpoint**: At this point, User Stories 1 AND 2 should both work - complete pipeline from image generation ‚Üí IPFS upload ‚Üí batch reveal on-chain

---

## Phase 5: User Story 3 - Resilient Error Handling (Priority: P2)

**Goal**: Automatically retry transient failures and clearly mark permanent failures so pipeline remains operational

**Independent Test**: Simulate various error conditions (network timeouts, authentication failures, gas spikes) and verify transient errors trigger retries with exponential backoff while permanent errors are marked appropriately

### Implementation for User Story 3

- [ ] T039 [US3] Implement exponential backoff logic in IPFS upload worker: calculate delay as `min(base_delay * (2 ** (attempts - 1)) + random(0, 5), max_delay)` where base_delay=1s, max_delay=60s, apply backoff after TransientError before next token processing
- [ ] T040 [US3] Add retry budget check in IPFS upload worker: before processing token, verify `generation_attempts < 3` (max retry limit), if exceeded skip token (remains 'failed'), log warning with token_id
- [ ] T041 [US3] Implement startup recovery function in IPFS upload worker to detect orphaned tokens (status='uploading' after worker restart), reset to stable state (status unchanged but ready for retry), log recovery event with count
- [ ] T042 [US3] Implement startup recovery function in reveal worker to detect orphaned batches (pending transactions that may have confirmed while worker was down), query transaction receipts for pending tx_hashes, update reveal_transactions records and tokens accordingly
- [ ] T043 [US3] Add database row-level locking verification: ensure `FOR UPDATE SKIP LOCKED` is used in both `get_ready_for_upload()` and `get_ready_for_reveal()` repository methods with `ORDER BY token_id` for deadlock prevention
- [ ] T044 [US3] Enhance error messages for PermanentError cases to include actionable context: for 401/403 errors include "Check PINATA_JWT configuration", for reverts include "Verify token IDs and metadata URIs", for keeper wallet errors include "Check keeper balance"
- [ ] T045 [US3] Add timeout handling for IPFS requests: set 30-second timeout on all HTTP requests in PinataClient, catch timeout exceptions, classify as TransientError with message "Request timeout after 30s"
- [ ] T046 [US3] Add timeout handling for blockchain transactions: verify `wait_for_transaction_receipt()` timeout parameter is set to `transaction_timeout_seconds` config value, catch `TimeExhausted` exception, classify as TransientError with tx_hash logged

**Checkpoint**: All user stories should now be independently functional with production-grade error handling and recovery

---

## Phase 6: Polish & Cross-Cutting Concerns

**Purpose**: Documentation, validation, and cleanup

- [ ] T047 [P] Update `backend/.env.example` to document all new environment variables: PINATA_JWT, PINATA_GATEWAY, KEEPER_PRIVATE_KEY, KEEPER_GAS_STRATEGY, REVEAL_GAS_BUFFER, TRANSACTION_TIMEOUT_SECONDS, BATCH_REVEAL_WAIT_SECONDS, BATCH_REVEAL_MAX_TOKENS with comments explaining purpose and example values
- [ ] T048 [P] Update CLAUDE.md to add 003d section documenting IPFS Upload & Reveal feature with key files, configuration, worker lifecycle, monitoring, manual recovery, performance tuning, troubleshooting, and reference links
- [ ] T049 Test migration idempotency by running `alembic downgrade -3 && alembic upgrade head` twice and verifying no errors or duplicate objects
- [ ] T050 Verify configuration validation logic by starting app with missing PINATA_JWT and KEEPER_PRIVATE_KEY, confirm warning logs are emitted with helpful messages
- [ ] T051 Run manual validation from `specs/003-003d-ipfs-reveal/quickstart.md`: Test 1 (IPFS upload single token), Test 2 (batch reveal multiple tokens), Test 3 (IPFS upload failure), Test 4 (reveal transaction revert)
- [ ] T052 Verify structured logging by tailing logs and checking for all required event types: `worker.started`, `ipfs.*`, `reveal.*`, `worker.stopped` with complete field sets
- [ ] T053 Code review: verify session-per-token isolation in both workers (no shared state between token processing), graceful shutdown handling (CancelledError propagation), error classification consistency

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup (Phase 1) completion - BLOCKS all user stories
- **User Stories (Phase 3-5)**: All depend on Foundational (Phase 2) completion
  - User Story 1 can start immediately after Phase 2
  - User Story 2 can start after Phase 2 (works independently but integrates with US1)
  - User Story 3 enhances both US1 and US2 (should follow their completion for validation)
- **Polish (Phase 6)**: Depends on all user stories (Phase 3-5) being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories, creates 'ready' tokens for US2
- **User Story 2 (P1)**: Can start after Foundational (Phase 2) - Depends on US1 creating 'ready' tokens but worker itself is independent
- **User Story 3 (P2)**: Enhances US1 and US2 - Should be implemented after US1 and US2 are working to validate recovery logic

### Within Each User Story

**User Story 1 (IPFS Upload)**:
- T016 (PinataClient) must complete before T021 (worker creation)
- T017-T019 (Pinata methods) can run in parallel with each other [P]
- T020 (build_metadata) can run in parallel with T016-T019 [P]
- T021-T024 (worker implementation) must follow T016-T020
- T025-T026 (registration/export) must follow T021-T024

**User Story 2 (Batch Reveal)**:
- T027 (KeeperService) must complete before T032 (worker creation)
- T028-T031 (keeper methods) should follow T027 sequentially
- T032-T036 (worker implementation) must follow T027-T031
- T037-T038 (registration/export) must follow T032-T036

**User Story 3 (Error Handling)**:
- All tasks can be implemented in parallel [P] once US1 and US2 are complete
- T039-T042 enhance worker logic (different functions)
- T043-T046 enhance error handling (different concerns)

### Parallel Opportunities

**Phase 1 (Setup)**:
- T001, T002, T003 can all run in parallel [P] - different files
- T004 must follow T002 (needs ServiceError base class)

**Phase 2 (Foundational)**:
- T005 (update Token model) must complete first
- T006, T007 (create new models) can run in parallel [P] after T005
- T008, T009, T010 (autogenerate migrations) can run in parallel [P] after T005-T007 complete
- T011 (review migrations) must follow T008-T010
- T012 (apply migrations) must follow T011
- T013, T014, T015 (repositories) can run in parallel [P] after T012

**Phase 3 (User Story 1)**:
- T016-T020 (services/utils) can run in parallel [P] - independent files
- T021-T024 (worker logic) are sequential - same file
- T025-T026 (integration) are sequential - same file

**Phase 4 (User Story 2)**:
- T027-T031 (KeeperService) are sequential - same file
- T032-T036 (worker logic) are sequential - same file
- T037-T038 (integration) are sequential - same file

**Phase 5 (User Story 3)**:
- T039-T046 can all run in parallel [P] - different functions/concerns

**Phase 6 (Polish)**:
- T047, T048 can run in parallel [P] - different files
- T049-T053 should run sequentially for validation

---

## Parallel Example: Foundational Phase

```bash
# STEP 1: Update models FIRST (models are source of truth per constitution v1.1.0)
Task: "Update Token model with new fields"  # T005

# STEP 2: Launch new model creation in parallel:
Task: "Create IPFSUploadRecord model"  # T006 [P]
Task: "Create RevealTransaction model"  # T007 [P]

# STEP 3: Autogenerate all three migrations in parallel:
Task: "Autogenerate migration add_ipfs_reveal_fields"  # T008 [P]
Task: "Autogenerate migration create_ipfs_upload_records"  # T009 [P]
Task: "Autogenerate migration create_reveal_transactions"  # T010 [P]

# STEP 4: Review autogenerated migrations:
Task: "Review all three migrations for correctness"  # T011

# STEP 5: Apply migrations sequentially:
Task: "Apply all migrations using alembic upgrade head"  # T012

# STEP 6: Launch all three repository files in parallel:
Task: "Create IPFSUploadRecordRepository"  # T013 [P]
Task: "Create RevealTransactionRepository"  # T014 [P]
Task: "Extend TokenRepository"  # T015
```

---

## Implementation Strategy

### MVP First (User Stories 1 + 2)

1. Complete Phase 1: Setup (dependencies, error hierarchy, config)
2. Complete Phase 2: Foundational (CRITICAL - database schema, models, repositories)
3. Complete Phase 3: User Story 1 (IPFS upload worker)
4. **STOP and VALIDATE**: Test US1 - token transitions to 'ready' with CIDs
5. Complete Phase 4: User Story 2 (batch reveal worker)
6. **STOP and VALIDATE**: Test US1+US2 - end-to-end pipeline works
7. Deploy/demo if ready

### Incremental Delivery

1. Complete Setup + Foundational ‚Üí Foundation ready
2. Add User Story 1 ‚Üí Test independently ‚Üí Tokens upload to IPFS
3. Add User Story 2 ‚Üí Test independently ‚Üí Tokens revealed on-chain (MVP complete!)
4. Add User Story 3 ‚Üí Test independently ‚Üí Production-grade error handling
5. Complete Polish ‚Üí Documentation and validation
6. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together (T001-T015)
2. Once Foundational is done:
   - **Developer A**: User Story 1 (T016-T026) - IPFS upload worker
   - **Developer B**: User Story 2 (T027-T038) - Reveal worker
   - Both can work in parallel - different files, shared foundation
3. Developer C (or A/B after completion): User Story 3 (T039-T046)
4. Team: Polish phase together (T047-T053)

---

## Notes

- [P] tasks = different files, no dependencies - can run in parallel
- [Story] label maps task to specific user story (US1, US2, US3)
- Each user story should be independently completable and testable
- Tests are NOT included (not requested in spec.md)
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Session-per-token pattern critical for both workers (no shared state)
- FOR UPDATE SKIP LOCKED required in both repository methods
- Graceful shutdown via asyncio.CancelledError propagation
- UTC enforcement in all migrations (server_default=CURRENT_TIMESTAMP)
