# Data Model: Simplified Token Recovery via nextTokenId

**Branch**: `004-recovery-1-nexttokenid` | **Date**: 2025-10-18

## Overview

This feature simplifies the Token entity by removing fields that cannot be populated during recovery. The data model changes focus on minimalism: store only what is essential and obtainable.

## Entity Changes

### Token (Modified)

**Table**: `tokens_s0`

**Purpose**: Represents a minted NFT with lifecycle status tracking through the pipeline (detection → generation → upload → reveal).

**Fields Before**:
- `id`: UUID (primary key)
- `token_id`: int (unique, indexed) - on-chain token ID
- `author_id`: UUID (foreign key to authors.id)
- **`minter_address`**: str(42) - **REMOVED**
- `status`: TokenStatus enum (indexed)
- **`mint_timestamp`**: datetime (indexed) - **REMOVED**
- `image_cid`: str(255) | null
- `metadata_cid`: str(255) | null
- `error_data`: JSONB | null
- `created_at`: datetime
- `image_url`: str | null
- `generation_attempts`: int
- `generation_error`: str(1000) | null
- `reveal_tx_hash`: str(66) | null

**Fields After** (removed fields marked):
- `id`: UUID (primary key)
- `token_id`: int (unique, indexed) - on-chain token ID
- `author_id`: UUID (foreign key to authors.id)
- ~~`minter_address`~~ - **REMOVED**
- `status`: TokenStatus enum (indexed)
- ~~`mint_timestamp`~~ - **REMOVED**
- `image_cid`: str(255) | null
- `metadata_cid`: str(255) | null
- `error_data`: JSONB | null
- `created_at`: datetime (automatic, not user-facing)
- `image_url`: str | null
- `generation_attempts`: int
- `generation_error`: str(1000) | null
- `reveal_tx_hash`: str(66) | null

**Rationale for Removals**:
1. **`minter_address`**: Not retrievable from on-chain data without parsing events. Not used in current pipeline (image generation, IPFS upload, reveal). Can be re-added if needed for analytics.
2. **`mint_timestamp`**: Not retrievable from token_id alone. Would require parsing event logs (defeating purpose of simplified recovery). Ordering can use `created_at` instead for most use cases.

**Impact on Queries**:
- Image generation worker: Previously used `ORDER BY mint_timestamp ASC` to process oldest tokens first. **Change to `ORDER BY created_at ASC`** (preserves FIFO behavior for webhook-detected tokens, acceptable for recovered tokens).
- No other queries rely on these fields (verified via grep).

**Validation Rules** (unchanged):
- `token_id`: Must be non-negative integer, unique across table
- `author_id`: Must reference existing author in authors table
- `status`: Must be valid TokenStatus enum value

**State Transitions** (unchanged):
```
detected → generating → uploading → ready → revealed
   ↓           ↓           ↓          ↓
 failed      failed      failed   failed
```

---

### Smart Contract State (Read-Only)

**Contract**: `GliskNFT.sol`

**Purpose**: Source of truth for total tokens minted. Recovery queries this to determine expected token range.

**Relevant State Variables**:
- `_nextTokenId`: uint256 (private) - Sequential counter starting at 1, increments after each mint
- `tokenPromptAuthor`: mapping(uint256 => address) - Maps token_id to prompt author wallet

**New Public Interface**:
```solidity
function nextTokenId() external view returns (uint256)
```

**Returns**: The next token ID that will be assigned on the next mint.

**Usage**: If `nextTokenId() == 11`, tokens 1-10 have been minted. **Token IDs start at 1** (not 0).

**Note**: This is a read-only operation. Recovery does NOT modify contract state.

---

## Database Schema Changes

### Migration: `remove_unused_recovery_fields`

**Upgrade** (autogenerated by Alembic):
```sql
ALTER TABLE tokens_s0 DROP COLUMN mint_timestamp;
ALTER TABLE tokens_s0 DROP COLUMN minter_address;
```

**Downgrade** (manually added for rollback safety):
```sql
ALTER TABLE tokens_s0 ADD COLUMN mint_timestamp TIMESTAMP;
ALTER TABLE tokens_s0 ADD COLUMN minter_address VARCHAR(42);
CREATE INDEX ix_tokens_s0_mint_timestamp ON tokens_s0(mint_timestamp);
```

**Notes**:
- Downgrade recreation of columns will have NULL values for existing rows (data loss acceptable per spec)
- Index on mint_timestamp recreated for consistency, but not strictly necessary if rolling back
- minter_address validation removed from model, so rollback would allow invalid addresses

---

## Repository Changes

### TokenRepository

**New Method**:
```python
async def get_missing_token_ids(self, max_token_id: int, limit: int | None = None) -> list[int]:
    """Retrieve token IDs that exist on-chain but not in database.

    Uses generate_series() to create expected range [0, max_token_id-1],
    then LEFT JOIN to find missing IDs.

    Args:
        max_token_id: Upper bound from contract.nextTokenId() (exclusive)
        limit: Optional cap on number of results (for batching large gaps)

    Returns:
        List of missing token IDs in ascending order

    Example:
        If max_token_id=11 (tokens 1-10 should exist) and DB has [1,2,3,6,7,8],
        returns [4,5,9,10]
    """
```

**SQL Implementation**:
```sql
SELECT series.token_id
FROM generate_series(1, :max_token_id - 1) AS series(token_id)
LEFT JOIN tokens_s0 ON series.token_id = tokens_s0.token_id
WHERE tokens_s0.token_id IS NULL
ORDER BY series.token_id ASC
LIMIT :limit;  -- LIMIT clause optional based on parameter
```

**Note**: Token IDs start at 1 (not 0), so generate_series starts at 1.

**Modified Method**:
```python
async def get_pending_for_generation(self, limit: int = 10) -> list[Token]:
    # OLD: .order_by(Token.mint_timestamp.asc())
    # NEW: .order_by(Token.created_at.asc())
```

---

## Data Flow: Recovery Process

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. Query Smart Contract                                         │
│    contract.functions.nextTokenId().call() → max_token_id       │
└────────────────────┬────────────────────────────────────────────┘
                     │
                     v
┌─────────────────────────────────────────────────────────────────┐
│ 2. Query Database for Missing IDs                               │
│    get_missing_token_ids(max_token_id) → [4, 5, 9, 10]         │
└────────────────────┬────────────────────────────────────────────┘
                     │
                     v
┌─────────────────────────────────────────────────────────────────┐
│ 3. Query Author Addresses from Contract                         │
│    For each missing_id:                                         │
│      contract.tokenPromptAuthor(missing_id) → author_wallet     │
│    Lookup/create authors in DB by wallet addresses              │
└────────────────────┬────────────────────────────────────────────┘
                     │
                     v
┌─────────────────────────────────────────────────────────────────┐
│ 4. Create Token Records                                         │
│    For each missing_id:                                         │
│      Token(                                                     │
│        token_id=missing_id,                                     │
│        author_id=author.id,  # Actual author from contract      │
│        status=TokenStatus.DETECTED,                             │
│        created_at=utcnow()                                      │
│      )                                                          │
└────────────────────┬────────────────────────────────────────────┘
                     │
                     v
┌─────────────────────────────────────────────────────────────────┐
│ 5. Persist with Duplicate Handling                              │
│    try:                                                         │
│      uow.tokens.add(token)                                      │
│      uow.commit()                                               │
│    except IntegrityError:  # Webhook created it concurrently   │
│      log.info("token_already_exists")                           │
│      continue                                                   │
└─────────────────────────────────────────────────────────────────┘
```

**Result**: Missing tokens are now in `detected` status, ready for image generation worker to process.

---

## Constraints and Indexes

**Unchanged Constraints**:
- `tokens_s0.token_id` UNIQUE (prevents duplicate tokens)
- `tokens_s0.author_id` FOREIGN KEY → authors.id (enforces referential integrity)

**Unchanged Indexes**:
- `tokens_s0.token_id` (unique index)
- `tokens_s0.status` (for worker queries)

**Removed Index**:
- `tokens_s0.mint_timestamp` (dropped with column)

**Performance**: Existing indexes sufficient for recovery queries. `generate_series()` query uses `tokens_s0.token_id` index for LEFT JOIN.

---

## Testing Data Model

### Unit Tests
- Verify Token model no longer has `mint_timestamp` and `minter_address` attributes
- Verify `get_missing_token_ids()` returns correct gaps for various scenarios:
  - Empty database (all tokens missing)
  - Partial gaps (some tokens missing)
  - No gaps (all tokens present)
  - Single missing token
  - Large gap (1000+ missing tokens)

### Integration Tests
- Create tokens with gaps in testcontainer PostgreSQL
- Verify `generate_series()` query performance (<1s for 100k range)
- Verify UNIQUE constraint prevents duplicate token_id insertion
- Verify migration up/down cycle preserves database consistency

---

## Compatibility Notes

**Backward Compatibility**:
- Existing tokens in database: No data migration needed. Fields are simply dropped.
- Existing workers: Must update code to remove references to dropped fields **before** running migration.
- Existing queries: Grep all code for `mint_timestamp` and `minter_address` references, update to use `created_at` or remove.

**Forward Compatibility**:
- If fields need to be re-added in future (e.g., for analytics), can recreate with NULL values.
- Consider separate analytics table instead of polluting core Token model.

---

## Summary

This data model change **simplifies** the Token entity by removing two fields that add complexity without providing value for the current recovery mechanism. The change aligns with Constitution principle "Simplicity First" by removing unused data rather than adding conditional logic to handle NULL values. Database performance is maintained (actually improved by dropping unused index). Recovery process relies on minimal data: token_id and author_id, both of which are always obtainable.
